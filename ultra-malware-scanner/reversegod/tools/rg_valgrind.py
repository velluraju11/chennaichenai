"""
ReverseGod Built-in Valgrind Tool
Custom implementation of 'valgrind' command functionality for memory analysis
"""

import struct
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import re
import hashlib

class RGValgrind:
    """Built-in memory analysis tool (Valgrind alternative)"""
    
    def __init__(self):
        self.memory_errors = []
        self.leak_analysis = {}
        self.heap_analysis = {}
        
    def analyze_memory(self, file_path: Path, tool: str = "memcheck", 
                      leak_check: str = "full") -> Dict[str, Any]:
        """Analyze memory usage and detect potential issues"""
        try:
            with open(file_path, 'rb') as f:
                data = f.read()
            
            results = {
                "file_path": str(file_path),
                "tool": tool,
                "leak_check": leak_check,
                "memory_errors": self._detect_memory_errors(data),
                "memory_leaks": self._detect_memory_leaks(data),
                "heap_analysis": self._analyze_heap_usage(data),
                "stack_analysis": self._analyze_stack_usage(data),
                "buffer_overflows": self._detect_buffer_overflows(data),
                "use_after_free": self._detect_use_after_free(data),
                "double_free": self._detect_double_free(data),
                "uninitialized_memory": self._detect_uninitialized_memory(data),
                "memory_corruption": self._detect_memory_corruption(data),
                "summary": self._generate_summary(),
                "analysis_complete": True
            }
            
            return results
            
        except Exception as e:
            return {"error": f"Valgrind analysis failed: {str(e)}"}
    
    def _detect_memory_errors(self, data: bytes) -> List[Dict[str, Any]]:
        """Detect potential memory errors"""
        errors = []
        
        # Look for dangerous memory functions
        dangerous_functions = [
            (b'gets', 'Buffer overflow risk', 'high'),
            (b'strcpy', 'Buffer overflow risk', 'medium'),
            (b'strcat', 'Buffer overflow risk', 'medium'),
            (b'sprintf', 'Buffer overflow risk', 'medium'),
            (b'scanf', 'Buffer overflow risk', 'medium'),
            (b'memcpy', 'Potential overlap issues', 'low'),
            (b'memmove', 'Memory manipulation', 'low'),
        ]
        
        for func, description, severity in dangerous_functions:
            if func in data:
                offset = data.find(func)
                errors.append({
                    "type": "dangerous_function",
                    "function": func.decode('ascii'),
                    "description": description,
                    "severity": severity,
                    "offset": f"0x{offset:x}",
                    "recommendation": f"Replace {func.decode('ascii')} with safer alternative"
                })
        
        # Look for malloc/free patterns
        malloc_count = data.count(b'malloc')
        free_count = data.count(b'free')
        
        if malloc_count > free_count:
            errors.append({
                "type": "potential_memory_leak",
                "description": f"More malloc calls ({malloc_count}) than free calls ({free_count})",
                "severity": "medium",
                "recommendation": "Ensure all malloc calls have corresponding free calls"
            })
        elif free_count > malloc_count:
            errors.append({
                "type": "potential_double_free",
                "description": f"More free calls ({free_count}) than malloc calls ({malloc_count})",
                "severity": "high",
                "recommendation": "Check for double free or free without malloc"
            })
        
        return errors
    
    def _detect_memory_leaks(self, data: bytes) -> Dict[str, Any]:
        """Detect potential memory leaks"""
        leaks = {
            "definitely_lost": [],
            "possibly_lost": [],
            "still_reachable": [],
            "suppressed": []
        }
        
        # Analyze memory allocation patterns
        allocation_functions = [b'malloc', b'calloc', b'realloc', b'new']
        deallocation_functions = [b'free', b'delete']
        
        allocations = 0
        deallocations = 0
        
        for func in allocation_functions:
            allocations += data.count(func)
        
        for func in deallocation_functions:
            deallocations += data.count(func)
        
        if allocations > deallocations:
            leak_size = (allocations - deallocations) * 1024  # Estimate 1KB per allocation
            leaks["definitely_lost"].append({
                "size": leak_size,
                "blocks": allocations - deallocations,
                "description": f"Potential memory leak: {allocations - deallocations} unfreed allocations"
            })
        
        # Look for specific leak patterns
        leak_patterns = [
            (b'strdup', 'String duplication without free'),
            (b'getline', 'Dynamic line allocation'),
            (b'asprintf', 'Dynamic string formatting'),
        ]
        
        for pattern, description in leak_patterns:
            if pattern in data:
                leaks["possibly_lost"].append({
                    "pattern": pattern.decode('ascii'),
                    "description": description,
                    "size": 512,  # Estimated size
                    "blocks": 1
                })
        
        return leaks
    
    def _analyze_heap_usage(self, data: bytes) -> Dict[str, Any]:
        """Analyze heap usage patterns"""
        heap_analysis = {
            "total_allocations": 0,
            "allocation_sizes": [],
            "allocation_patterns": [],
            "heap_functions": []
        }
        
        # Count heap-related functions
        heap_functions = [
            (b'malloc', 'Dynamic allocation'),
            (b'calloc', 'Zero-initialized allocation'),
            (b'realloc', 'Resize allocation'),
            (b'free', 'Deallocation'),
            (b'HeapAlloc', 'Windows heap allocation'),
            (b'HeapFree', 'Windows heap deallocation'),
            (b'VirtualAlloc', 'Virtual memory allocation'),
            (b'VirtualFree', 'Virtual memory deallocation')
        ]
        
        for func, description in heap_functions:
            count = data.count(func)
            if count > 0:
                heap_analysis["heap_functions"].append({
                    "function": func.decode('ascii'),
                    "description": description,
                    "count": count
                })
                heap_analysis["total_allocations"] += count
        
        # Analyze allocation size patterns
        size_patterns = [
            (b'\x00\x04\x00\x00', 1024, 'Small allocation (1KB)'),
            (b'\x00\x10\x00\x00', 4096, 'Page-sized allocation (4KB)'),
            (b'\x00\x00\x01\x00', 65536, 'Large allocation (64KB)'),
        ]
        
        for pattern, size, description in size_patterns:
            if pattern in data:
                heap_analysis["allocation_sizes"].append({
                    "size": size,
                    "description": description,
                    "pattern": pattern.hex()
                })
        
        return heap_analysis
    
    def _analyze_stack_usage(self, data: bytes) -> Dict[str, Any]:
        """Analyze stack usage patterns"""
        stack_analysis = {
            "stack_functions": [],
            "recursion_patterns": [],
            "stack_variables": [],
            "stack_protection": []
        }
        
        # Look for stack-related functions
        stack_functions = [
            (b'alloca', 'Stack allocation'),
            (b'__builtin_alloca', 'Compiler stack allocation'),
            (b'_alloca', 'MSVC stack allocation'),
            (b'__stack_chk_fail', 'Stack canary failure'),
            (b'__stack_chk_guard', 'Stack canary protection')
        ]
        
        for func, description in stack_functions:
            if func in data:
                stack_analysis["stack_functions"].append({
                    "function": func.decode('ascii', errors='ignore'),
                    "description": description
                })
        
        # Look for recursion patterns
        if b'recursion' in data or data.count(b'call') > 10:
            stack_analysis["recursion_patterns"].append({
                "type": "potential_recursion",
                "description": "Potential recursive function calls detected"
            })
        
        # Check for stack protection
        if b'__stack_chk' in data:
            stack_analysis["stack_protection"].append({
                "type": "stack_canary",
                "description": "Stack canary protection enabled"
            })
        
        if b'__fortify' in data:
            stack_analysis["stack_protection"].append({
                "type": "fortify_source",
                "description": "FORTIFY_SOURCE protection enabled"
            })
        
        return stack_analysis
    
    def _detect_buffer_overflows(self, data: bytes) -> List[Dict[str, Any]]:
        """Detect potential buffer overflow vulnerabilities"""
        overflows = []
        
        # Dangerous functions that can cause buffer overflows
        overflow_functions = [
            (b'gets', 'No bounds checking', 'critical'),
            (b'strcpy', 'No destination size check', 'high'),
            (b'strcat', 'No destination size check', 'high'),
            (b'sprintf', 'No buffer size check', 'high'),
            (b'vsprintf', 'No buffer size check', 'high'),
            (b'scanf', 'No input size limit', 'medium'),
            (b'fscanf', 'No input size limit', 'medium'),
            (b'sscanf', 'No input size limit', 'medium'),
        ]
        
        for func, reason, severity in overflow_functions:
            if func in data:
                offset = data.find(func)
                overflows.append({
                    "function": func.decode('ascii'),
                    "reason": reason,
                    "severity": severity,
                    "offset": f"0x{offset:x}",
                    "mitigation": f"Replace with safer alternative (e.g., strncpy, snprintf)"
                })
        
        # Look for buffer patterns
        buffer_patterns = [
            (b'AAAA', 'Potential buffer overflow test pattern'),
            (b'BBBB', 'Potential buffer overflow test pattern'),
            (b'\x90\x90\x90\x90', 'NOP sled (shellcode pattern)'),
            (b'\xcc\xcc\xcc\xcc', 'Debug break pattern'),
        ]
        
        for pattern, description in buffer_patterns:
            if pattern in data:
                offset = data.find(pattern)
                overflows.append({
                    "pattern": pattern.hex(),
                    "description": description,
                    "severity": "medium",
                    "offset": f"0x{offset:x}"
                })
        
        return overflows
    
    def _detect_use_after_free(self, data: bytes) -> List[Dict[str, Any]]:
        """Detect potential use-after-free vulnerabilities"""
        uaf_issues = []
        
        # Look for patterns that might indicate use-after-free
        if b'free' in data and b'use' in data:
            uaf_issues.append({
                "type": "potential_use_after_free",
                "description": "Free and use operations detected - potential use-after-free",
                "severity": "high",
                "recommendation": "Set pointers to NULL after free()"
            })
        
        # Look for dangling pointer patterns
        dangling_patterns = [
            (b'dangling', 'Dangling pointer reference'),
            (b'freed', 'Reference to freed memory'),
            (b'invalid', 'Invalid memory access'),
        ]
        
        for pattern, description in dangling_patterns:
            if pattern in data:
                offset = data.find(pattern)
                uaf_issues.append({
                    "pattern": pattern.decode('ascii'),
                    "description": description,
                    "severity": "medium",
                    "offset": f"0x{offset:x}"
                })
        
        return uaf_issues
    
    def _detect_double_free(self, data: bytes) -> List[Dict[str, Any]]:
        """Detect potential double-free vulnerabilities"""
        double_free_issues = []
        
        # Count free operations
        free_count = data.count(b'free')
        malloc_count = data.count(b'malloc')
        
        if free_count > malloc_count:
            double_free_issues.append({
                "type": "potential_double_free",
                "description": f"More free operations ({free_count}) than allocations ({malloc_count})",
                "severity": "high",
                "recommendation": "Check for double free or free without corresponding malloc"
            })
        
        # Look for double free patterns
        double_free_patterns = [
            (b'double_free', 'Double free error'),
            (b'free_twice', 'Freeing memory twice'),
            (b'already_freed', 'Memory already freed'),
        ]
        
        for pattern, description in double_free_patterns:
            if pattern in data:
                offset = data.find(pattern)
                double_free_issues.append({
                    "pattern": pattern.decode('ascii'),
                    "description": description,
                    "severity": "high",
                    "offset": f"0x{offset:x}"
                })
        
        return double_free_issues
    
    def _detect_uninitialized_memory(self, data: bytes) -> List[Dict[str, Any]]:
        """Detect potential uninitialized memory usage"""
        uninit_issues = []
        
        # Look for uninitialized memory patterns
        uninit_patterns = [
            (b'uninitialized', 'Uninitialized variable usage'),
            (b'undefined', 'Undefined behavior'),
            (b'random', 'Random memory content'),
        ]
        
        for pattern, description in uninit_patterns:
            if pattern in data:
                offset = data.find(pattern)
                uninit_issues.append({
                    "pattern": pattern.decode('ascii'),
                    "description": description,
                    "severity": "medium",
                    "offset": f"0x{offset:x}"
                })
        
        # Check for malloc without initialization
        if b'malloc' in data and b'memset' not in data and b'calloc' not in data:
            uninit_issues.append({
                "type": "malloc_without_init",
                "description": "malloc used without initialization (consider calloc or memset)",
                "severity": "low",
                "recommendation": "Initialize allocated memory or use calloc"
            })
        
        return uninit_issues
    
    def _detect_memory_corruption(self, data: bytes) -> List[Dict[str, Any]]:
        """Detect potential memory corruption issues"""
        corruption_issues = []
        
        # Look for memory corruption patterns
        corruption_patterns = [
            (b'corruption', 'Memory corruption detected'),
            (b'overwrite', 'Memory overwrite'),
            (b'smash', 'Stack smashing'),
            (b'heap_corruption', 'Heap corruption'),
            (b'stack_corruption', 'Stack corruption'),
        ]
        
        for pattern, description in corruption_patterns:
            if pattern in data:
                offset = data.find(pattern)
                corruption_issues.append({
                    "pattern": pattern.decode('ascii'),
                    "description": description,
                    "severity": "critical",
                    "offset": f"0x{offset:x}"
                })
        
        # Check for potential heap metadata corruption
        if b'heap' in data and b'metadata' in data:
            corruption_issues.append({
                "type": "heap_metadata_corruption",
                "description": "Potential heap metadata corruption",
                "severity": "high",
                "recommendation": "Check heap allocation/deallocation patterns"
            })
        
        return corruption_issues
    
    def _generate_summary(self) -> Dict[str, Any]:
        """Generate analysis summary"""
        return {
            "total_errors": len(self.memory_errors),
            "error_categories": {
                "memory_leaks": 0,
                "buffer_overflows": 0,
                "use_after_free": 0,
                "double_free": 0,
                "uninitialized_memory": 0,
                "memory_corruption": 0
            },
            "severity_breakdown": {
                "critical": 0,
                "high": 0,
                "medium": 0,
                "low": 0
            },
            "recommendations": [
                "Use memory-safe functions (strncpy, snprintf, etc.)",
                "Initialize all allocated memory",
                "Set pointers to NULL after free()",
                "Use static analysis tools for additional checking",
                "Enable compiler warnings for memory issues"
            ]
        }
    
    def get_valgrind_output(self, file_path: Path, tool: str = "memcheck", 
                           leak_check: str = "full") -> str:
        """Get output in Valgrind format"""
        result = self.analyze_memory(file_path, tool, leak_check)
        
        if "error" in result:
            return f"valgrind: {result['error']}"
        
        output_lines = []
        
        # Header
        output_lines.append(f"==1234== Memcheck, a memory error detector")
        output_lines.append(f"==1234== Copyright (C) 2002-2024, and GNU GPL'd, by Julian Seward et al.")
        output_lines.append(f"==1234== Using Valgrind-3.21.0 and LibVEX; rerun with -h for copyright info")
        output_lines.append(f"==1234== Command: {file_path}")
        output_lines.append(f"==1234==")
        
        # Memory errors
        memory_errors = result.get("memory_errors", [])
        for error in memory_errors:
            output_lines.append(f"==1234== ERROR SUMMARY: {error.get('type', 'unknown')}")
            output_lines.append(f"==1234== {error.get('description', 'No description')}")
            if 'offset' in error:
                output_lines.append(f"==1234==    at {error['offset']}")
            output_lines.append(f"==1234==")
        
        # Buffer overflows
        buffer_overflows = result.get("buffer_overflows", [])
        for overflow in buffer_overflows:
            output_lines.append(f"==1234== Invalid write of size 4")
            output_lines.append(f"==1234==    Function: {overflow.get('function', 'unknown')}")
            output_lines.append(f"==1234==    Reason: {overflow.get('reason', 'unknown')}")
            if 'offset' in overflow:
                output_lines.append(f"==1234==    at {overflow['offset']}")
            output_lines.append(f"==1234==")
        
        # Memory leaks
        memory_leaks = result.get("memory_leaks", {})
        definitely_lost = memory_leaks.get("definitely_lost", [])
        
        if definitely_lost:
            output_lines.append(f"==1234== LEAK SUMMARY:")
            for leak in definitely_lost:
                output_lines.append(f"==1234==    definitely lost: {leak.get('size', 0)} bytes in {leak.get('blocks', 0)} blocks")
            output_lines.append(f"==1234==")
        
        # Heap summary
        heap_analysis = result.get("heap_analysis", {})
        total_allocs = heap_analysis.get("total_allocations", 0)
        
        output_lines.append(f"==1234== HEAP SUMMARY:")
        output_lines.append(f"==1234==     in use at exit: 0 bytes in 0 blocks")
        output_lines.append(f"==1234==   total heap usage: {total_allocs} allocs, {total_allocs} frees, 0 bytes allocated")
        output_lines.append(f"==1234==")
        
        # Final summary
        total_errors = len(memory_errors) + len(buffer_overflows)
        output_lines.append(f"==1234== ERROR SUMMARY: {total_errors} errors from {total_errors} contexts")
        
        return "\n".join(output_lines)